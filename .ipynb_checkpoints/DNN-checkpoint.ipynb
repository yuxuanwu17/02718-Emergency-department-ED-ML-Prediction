{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the feature selected data frame via mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534730, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(33)\n",
    "df = pd.read_csv(\"data/featureSelectedAllDataWithY.csv\")\n",
    "# df = pd.read_csv(\"../data/Cleaned_dat_encoded.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the label to 0,1 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         1\n",
      "         ..\n",
      "534725    1\n",
      "534726    1\n",
      "534727    0\n",
      "534728    1\n",
      "534729    1\n",
      "Name: disposition, Length: 534730, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['disposition'].replace(2,0,inplace=True)\n",
    "print(df['disposition'])\n",
    "## Split the data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data, testing_data = train_test_split(df, test_size=0.2, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "y_train = training_data['disposition']\n",
    "y_test = testing_data['disposition']\n",
    "X_train = StandardScaler().fit_transform(training_data.drop(\"disposition\",axis = 1))\n",
    "X_test = StandardScaler().fit_transform(testing_data.drop(\"disposition\",axis = 1))\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "# y_train = np.array(y_train)[:100]\n",
    "# y_test = np.array(y_test)[:10]\n",
    "# X_train = np.array(X_train)[:100,]\n",
    "# X_test = np.array(X_test)[:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        Dense(30, activation='relu',input_shape=(20,)),\n",
    "        Dense(30, activation='relu'),\n",
    "        Dense(30, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy' , metrics = ['accuracy'],weighted_metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with parameters in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 427784 samples\n",
      "Epoch 1/30\n",
      "427784/427784 [==============================] - 11s 26us/sample - loss: 0.3662 - accuracy: 0.8419 - accuracy_1: 0.8419\n",
      "Epoch 2/30\n",
      "427784/427784 [==============================] - 11s 25us/sample - loss: 0.3578 - accuracy: 0.8451 - accuracy_1: 0.8451\n",
      "Epoch 3/30\n",
      "427784/427784 [==============================] - 10s 24us/sample - loss: 0.3557 - accuracy: 0.8461 - accuracy_1: 0.8461\n",
      "Epoch 4/30\n",
      "427784/427784 [==============================] - 10s 24us/sample - loss: 0.3532 - accuracy: 0.8477 - accuracy_1: 0.8477\n",
      "Epoch 5/30\n",
      "427784/427784 [==============================] - 10s 24us/sample - loss: 0.3519 - accuracy: 0.8483 - accuracy_1: 0.8483\n",
      "Epoch 6/30\n",
      "427784/427784 [==============================] - 11s 25us/sample - loss: 0.3513 - accuracy: 0.8485 - accuracy_1: 0.8485\n",
      "Epoch 7/30\n",
      "427784/427784 [==============================] - 20s 48us/sample - loss: 0.3511 - accuracy: 0.8490 - accuracy_1: 0.8490\n",
      "Epoch 8/30\n",
      "427784/427784 [==============================] - 14s 32us/sample - loss: 0.3510 - accuracy: 0.8491 - accuracy_1: 0.8491\n",
      "Epoch 9/30\n",
      "427784/427784 [==============================] - 11s 26us/sample - loss: 0.3509 - accuracy: 0.8493 - accuracy_1: 0.8493\n",
      "Epoch 10/30\n",
      "427784/427784 [==============================] - 14s 32us/sample - loss: 0.3507 - accuracy: 0.8494 - accuracy_1: 0.8494\n",
      "Epoch 11/30\n",
      "427784/427784 [==============================] - 14s 34us/sample - loss: 0.3508 - accuracy: 0.8491 - accuracy_1: 0.8491\n",
      "Epoch 12/30\n",
      "427784/427784 [==============================] - 14s 33us/sample - loss: 0.3507 - accuracy: 0.8493 - accuracy_1: 0.8493\n",
      "Epoch 13/30\n",
      "427784/427784 [==============================] - 13s 31us/sample - loss: 0.3506 - accuracy: 0.8493 - accuracy_1: 0.8493\n",
      "Epoch 14/30\n",
      "427784/427784 [==============================] - 12s 29us/sample - loss: 0.3506 - accuracy: 0.8493 - accuracy_1: 0.8493\n",
      "Epoch 15/30\n",
      "427784/427784 [==============================] - 14s 33us/sample - loss: 0.3508 - accuracy: 0.8495 - accuracy_1: 0.8495\n",
      "Epoch 16/30\n",
      "427784/427784 [==============================] - 13s 30us/sample - loss: 0.3508 - accuracy: 0.8493 - accuracy_1: 0.8493\n",
      "Epoch 17/30\n",
      "427784/427784 [==============================] - 13s 30us/sample - loss: 0.3509 - accuracy: 0.8491 - accuracy_1: 0.8491\n",
      "Epoch 18/30\n",
      "427784/427784 [==============================] - 14s 32us/sample - loss: 0.3511 - accuracy: 0.8493 - accuracy_1: 0.8493\n",
      "Epoch 19/30\n",
      "427784/427784 [==============================] - 14s 33us/sample - loss: 0.3510 - accuracy: 0.8494 - accuracy_1: 0.8494\n",
      "Epoch 20/30\n",
      "427784/427784 [==============================] - 13s 31us/sample - loss: 0.3511 - accuracy: 0.8493 - accuracy_1: 0.8493\n",
      "Epoch 21/30\n",
      "427784/427784 [==============================] - 14s 33us/sample - loss: 0.3510 - accuracy: 0.8495 - accuracy_1: 0.8495\n",
      "Epoch 22/30\n",
      "427784/427784 [==============================] - 14s 32us/sample - loss: 0.3509 - accuracy: 0.8492 - accuracy_1: 0.8492\n",
      "Epoch 23/30\n",
      "427784/427784 [==============================] - 13s 30us/sample - loss: 0.3510 - accuracy: 0.8492 - accuracy_1: 0.8492\n",
      "Epoch 24/30\n",
      "427784/427784 [==============================] - 14s 32us/sample - loss: 0.3512 - accuracy: 0.8494 - accuracy_1: 0.8494\n",
      "Epoch 25/30\n",
      "427784/427784 [==============================] - 13s 31us/sample - loss: 0.3512 - accuracy: 0.8493 - accuracy_1: 0.8493\n",
      "Epoch 26/30\n",
      "427784/427784 [==============================] - 14s 33us/sample - loss: 0.3513 - accuracy: 0.8494 - accuracy_1: 0.8494\n",
      "Epoch 27/30\n",
      "427784/427784 [==============================] - 15s 36us/sample - loss: 0.3512 - accuracy: 0.8492 - accuracy_1: 0.8492\n",
      "Epoch 28/30\n",
      "427784/427784 [==============================] - 18s 43us/sample - loss: 0.3512 - accuracy: 0.8495 - accuracy_1: 0.8495\n",
      "Epoch 29/30\n",
      "427784/427784 [==============================] - 15s 36us/sample - loss: 0.3515 - accuracy: 0.8497 - accuracy_1: 0.8497\n",
      "Epoch 30/30\n",
      "427784/427784 [==============================] - 16s 38us/sample - loss: 0.3515 - accuracy: 0.8494 - accuracy_1: 0.8494\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[70013  4098]\n",
      " [11784 21051]]\n",
      "0.8514951470835749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "np.random.seed(52)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow_core.python.keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# batch_size = best_clf.best_params_['batch_size']\n",
    "# epochs = best_clf.best_params_['epochs']\n",
    "# optimizer = best_clf.best_params_['optimizer']\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "optimizer = 'RMSprop'\n",
    "\n",
    "clf = create_model(optimizer)\n",
    "history = clf.fit(X_train, y_train,batch_size=batch_size,epochs=epochs)\n",
    "clf.save(\"dnn.h5\")\n",
    "# clf.fit(X_train, y_train,batch_size=batch_size,epochs=epochs)\n",
    "y_pred = clf.predict_classes(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "mylist = []\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "mylist.append(ac)\n",
    "print(cm)\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Grid Search for DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [16, 32, 64]\n",
    "epochs = [20, 50, 80, 110]\n",
    "optimizer = ['SGD', 'Adam','RMSprop']\n",
    "param_grid = dict(batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "best_clf = clf.fit(X_train, y_train)np.random.seed(52)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow_core.python.keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# create model\n",
    "model =KerasClassifier(build_fn=create_model,verbose=0)\n",
    "\n",
    "## add the compile method for the neural network\n",
    "\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=5,n_jobs=-1)\n",
    "# clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=5,n_jobs=-1)\n",
    "best_clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best cross validation accuracy: {:.2f}\".format(best_clf.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(best_clf.score(X_test,y_test)))\n",
    "print(\"Best parameters: {}\".format(best_clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prediction of the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "np.random.seed(52)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow_core.python.keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "batch_size = best_clf.best_params_['batch_size']\n",
    "epochs = best_clf.best_params_['epochs']\n",
    "optimizer = best_clf.best_params_['optimizer']\n",
    "\n",
    "# batch_size = 128\n",
    "# epochs = 30\n",
    "# optimizer = 'RMSprop'\n",
    "\n",
    "clf = create_model(optimizer)\n",
    "history = clf.fit(X_train, y_train,batch_size=batch_size,epochs=epochs)\n",
    "clf.save(\"dnn.h5\")\n",
    "# clf.fit(X_train, y_train,batch_size=batch_size,epochs=epochs)\n",
    "y_pred = clf.predict_classes(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "mylist = []\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "mylist.append(ac)\n",
    "print(cm)\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the matrix results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+-----------+--------+--------+\n",
      "| Accuracy | auROC  | auPRC  | recall | precision |   f1   |  MCC   |\n",
      "+----------+--------+--------+--------+-----------+--------+--------+\n",
      "|  0.8515  | 0.9033 | 0.8345 | 0.6411 |   0.8371  | 0.7261 | 0.6372 |\n",
      "+----------+--------+--------+--------+-----------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from prettytable import PrettyTable\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "MCCs = []\n",
    "auROCs = []\n",
    "auPRCs = []\n",
    "\n",
    "accuracy_scores.append(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "f1_scores.append(f1_score(y_true=y_test, y_pred=y_pred))\n",
    "recall_scores.append(recall_score(y_true=y_test, y_pred=y_pred))\n",
    "precision_scores.append(precision_score(y_true=y_test, y_pred=y_pred))\n",
    "MCCs.append(matthews_corrcoef(y_true=y_test, y_pred=y_pred))\n",
    "auROCs.append(roc_auc_score(y_true=y_test, y_score=clf.predict(X_test)))\n",
    "auPRCs.append(average_precision_score(y_true=y_test,  y_score=clf.predict(X_test)))\n",
    "\n",
    "table = PrettyTable()\n",
    "column_names = ['Accuracy', 'auROC', 'auPRC', 'recall', 'precision', 'f1', 'MCC']\n",
    "table.add_column(column_names[0], np.round(accuracy_scores, 4))\n",
    "table.add_column(column_names[1], np.round(auROCs, 4))\n",
    "table.add_column(column_names[2], np.round(auPRCs, 4))\n",
    "table.add_column(column_names[3], np.round(recall_scores, 4))\n",
    "table.add_column(column_names[4], np.round(precision_scores, 4))\n",
    "table.add_column(column_names[5], np.round(f1_scores, 4))\n",
    "table.add_column(column_names[6], np.round(MCCs, 4))\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (proj4CM)",
   "language": "python",
   "name": "pycharm-f3dbbc4a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
